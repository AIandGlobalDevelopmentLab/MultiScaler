{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3233103-58b3-48e5-b237-72c01e3013b8",
   "metadata": {},
   "source": [
    "Similar to MultiScaleSimFull, but also quantifying the $R^2$ of the MLP model on perturbed data. See MultiScaleSimFull for a full description of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec4e6aa-2aa4-4580-9bf1-b53432ef4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Running in Jupyter Notebook. Using default value -1.\n",
      "Running CLIP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set your working directory\n",
    "wd = \"./Sim\"\n",
    "os.chdir(wd)\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import copy\n",
    "import logging \n",
    "import time\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFilter, ImageOps, ImageEnhance\n",
    "from collections import defaultdict\n",
    "from Utils import process_images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoImageProcessor, SwinModel, set_seed\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "# Global Variables\n",
    "MODEL_TYPE = 'CLIP' # 'CLIP', 'SWIN', \n",
    "CLIP=True\n",
    "SWIN=!CLIP\n",
    "\n",
    "assert CLIP != SWIN\n",
    "REDOWNLOADDATA = False\n",
    "Separate = False\n",
    "Full=True\n",
    "target_name = 'NOTobsY' # 'obsY', or 'tauPred'\n",
    "image_sizes = [32, 256]\n",
    "monte_is = [0, 1, 2, 3, 4, 5]\n",
    "NUM_IMAGE_SIZES = len(image_sizes)\n",
    "\n",
    "# Path parameters\n",
    "optimize_image_reps = \"clip-rsicd\"\n",
    "data_type = \"image\"\n",
    "perturbation_type = \"DHS\"\n",
    "applications = [\"peru\"]\n",
    "perturbation_magnitude = 0\n",
    "image_type = \"original\"\n",
    "# For privacy preserving visualisation\n",
    "if False:\n",
    "    perturbation_type = \"deterministic\"\n",
    "    applications = [\"peru\"]\n",
    "    perturbation_magnitude = 10\n",
    "    image_type = \"cluster\"\n",
    "    \n",
    "if sys.argv[1] == '-f':\n",
    "    iter_num = -1\n",
    "    print(\"Warning: Running in Jupyter Notebook. Using default value -1.\")\n",
    "elif len(sys.argv) > 1:\n",
    "    args = sys.argv[1:]  # Get command line arguments\n",
    "    iter_num = int(args[0])  # Assuming args is defined earlier in the code\n",
    "    monte_carlo_i = int(args[1])\n",
    "    if len(sys.argv) > 3:\n",
    "        image_size1 = args[2]\n",
    "        image_size2 = args[3]\n",
    "else:\n",
    "    iter_num = -1\n",
    "    print(\"Warning: No iteration number provided. Using default value -1.\")\n",
    "\n",
    "# Directories\n",
    "os.makedirs('./Figures', exist_ok=True)\n",
    "os.makedirs('./TrainingInfo', exist_ok=True)\n",
    "os.makedirs('./log', exist_ok=True)\n",
    "os.makedirs('./FineTuneIterResults', exist_ok=True)\n",
    "save_ImageTensor_dir = './DataInfo/Images'\n",
    "saveObsWDir = './DataInfo/ObsW'\n",
    "saveGeoDir = './DataInfo/geo'\n",
    "saveObsYDir = './DataInfo/ObsY'\n",
    "saveTrainIndicesDir = './DataInfo/TrainIndices'\n",
    "saveResDir = './FineTuneIterResults/FineTuneRes'\n",
    "saveRepDir = './FineTuneIterResults/FineTuneReps'\n",
    "\n",
    "saveDirs = [saveObsWDir, saveGeoDir, saveObsYDir, save_ImageTensor_dir, saveTrainIndicesDir, saveResDir, saveResDir]\n",
    "for dir in saveDirs:\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "logging.basicConfig(filename=f'./log/trainingIter_{iter_num}.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "def set_all_seeds(seed_value=42):\n",
    "    # 1. Python's built-in random library\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "    # 2. NumPy's random seed\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "set_all_seeds(1)\n",
    "\n",
    "# Conditional loading of models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if CLIP:\n",
    "    # Load CLIP model and processor\n",
    "    backbone_model = CLIPModel.from_pretrained(\"flax-community/clip-rsicd-v2\").to(device)\n",
    "    backbone_processor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd-v2\")\n",
    "    print(\"Running CLIP\")\n",
    "\n",
    "elif SWIN:\n",
    "    # Load Swin model and processor\n",
    "    backbone_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "    backbone_model = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\").to(device)\n",
    "    print(\"Running SWIN\")\n",
    "\n",
    "else:\n",
    "    # Raise an error if neither model is selected\n",
    "    raise ValueError(\"You need to specify a valid backbone model: CLIP or SWIN.\")\n",
    "#from transformers import ViTMAEModel, AutoImageProcessor\n",
    "#backbone_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "#backbone_model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563ec1c-75b0-48d6-a6d5-40689377d0d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697fa9c1-1ca4-47ae-9625-619f5efefd04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'applications' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m perturbation_magnitude \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m monte_is \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m application \u001b[38;5;129;01min\u001b[39;00m \u001b[43mapplications\u001b[49m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_size \u001b[38;5;129;01min\u001b[39;00m image_sizes:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m monte_carlo_i \u001b[38;5;129;01min\u001b[39;00m monte_is:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'applications' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize lists and dictionaries\n",
    "targets = []\n",
    "application_len = {}\n",
    "application_image_ids = {}\n",
    "ObsW = {}\n",
    "ObsY = {}\n",
    "geo = {}\n",
    "results_list = []\n",
    "pil_images_list = []\n",
    "sampled_images_list = []\n",
    "perturbation_magnitude = 0\n",
    "monte_is = [1, 0, 2, 3, 4, 5]\n",
    "for application in applications:\n",
    "    for image_size in image_sizes:\n",
    "        for monte_carlo_i in monte_is:\n",
    "            if image_size == 256 and monte_carlo_i != 1: \n",
    "                break\n",
    "            if monte_carlo_i == 1:\n",
    "                perturbation_magnitude = 0\n",
    "            else:\n",
    "                perturbation_magnitude = 7\n",
    "                \n",
    "            # Define image directory with a single image size\n",
    "            image_size = image_size\n",
    "            # Get image directory path\n",
    "            image_dir = f'./Data/{application}/monte_carlo_{monte_carlo_i}/satellite_images/landsat/{perturbation_type}/{image_size}/30/{image_type}_{application}/magnitude_{perturbation_magnitude}/'\n",
    "            my_data_path = \"./Data/Joint/pooled_hh.dta\"\n",
    "            geo_path = f\"./Data/{application}/master/gps_locations/{application}_cluster_center_coordinates_per_person_fully_subsetted.csv\"\n",
    "            \n",
    "            # Where the results live\n",
    "            results_path = f\"./SavedResults/{application}/monte_carlo_1/{optimize_image_reps}/landsat/DHS/{image_size}/30/asset_hh_index_diff/rs_nDepthIS1_analysisTypeISheterogeneity_imageModelClassISVisionTransformer_optimizeImageRepIS{optimize_image_reps}_dataTypeISimage_applicationIS{application}_perturbationMagnitudeIS0_monte_carlo_iIS1_original_10.csv\"\n",
    "\n",
    "            # Load and preprocess data\n",
    "            geo_df = pd.read_csv(geo_path)\n",
    "            my_data = pd.read_stata(my_data_path)  \n",
    "            new_columns = pd.DataFrame({\n",
    "                'obsY': my_data['asset_hh_index_end'] - my_data['asset_hh_index_bsl'],\n",
    "                'treatment_indicator': my_data['treatment'].map({'Control': 0, 'Treatment': 1}),\n",
    "            })\n",
    "            my_data = pd.concat([my_data, new_columns], axis=1)\n",
    "            my_data = my_data[my_data['id'].astype(str).isin(geo_df['FPrimary'].astype(str))]\n",
    "            my_data = my_data[my_data['obsY'].notna()].reset_index(drop=True)\n",
    "\n",
    "            if len(my_data) % 2 != 0: \n",
    "                my_data = my_data.iloc[:-1]\n",
    "\n",
    "            total_size = len(my_data)\n",
    "            application_len[application] = total_size\n",
    "\n",
    "            # Assign all indices for processing without train-test split\n",
    "            all_indices = list(my_data.index)\n",
    "\n",
    "            # Load outcome targets\n",
    "            results_df = pd.read_csv(results_path)\n",
    "            new_targets = results_df['tau_est'].values\n",
    "            targets.extend(list(new_targets[all_indices]))\n",
    "\n",
    "            id_with_outcome = my_data['id'].astype(str).values\n",
    "            assert (len(id_with_outcome) % 2) == 0, \"ID count is not even.\"\n",
    "\n",
    "            # Function to extract ID from filename\n",
    "            def extract_id(filename):\n",
    "                return filename.split(\"-\")[0]\n",
    "\n",
    "            # Get sorted list of image paths based on IDs\n",
    "            subsetted_image_paths = [file for file in os.listdir(image_dir) if extract_id(file) in id_with_outcome]\n",
    "\n",
    "            # Create a mapping from ID to filename\n",
    "            id_to_filename = {extract_id(file): file for file in subsetted_image_paths}\n",
    "\n",
    "            # Ensure all IDs have corresponding images\n",
    "            missing_ids = set(id_with_outcome) - set(id_to_filename.keys())\n",
    "\n",
    "            if missing_ids:\n",
    "                raise ValueError(f\"Missing images in image_dir for IDs: {missing_ids}\")\n",
    "\n",
    "            # Process images in the order of id_with_outcome\n",
    "            pil_images_app = process_images(image_dir, [id_to_filename[id] for id in id_with_outcome])\n",
    "            \n",
    "            if monte_carlo_i == 1:\n",
    "                application_image_ids[(application, image_size)] = list(my_data['id'].astype(str))\n",
    "\n",
    "                # Get auxiliary info for regression\n",
    "                ObsW[(application, image_size)] = my_data['treatment_indicator']\n",
    "                ObsY[(application, image_size)] = my_data['obsY']\n",
    "                geo[(application, image_size)] = my_data['geo']\n",
    "                pil_images_list.append(pil_images_app)\n",
    "                results_list.append(results_df)\n",
    "            else:\n",
    "                sampled_images_list.append(pil_images_app)\n",
    "                print(f\"Saved sampled images of size {image_size} from {monte_carlo_i}th iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d9a9e-9e25-493a-b9f3-39476d4fc259",
   "metadata": {},
   "source": [
    "# Define Image Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637ea853-46d3-41b7-ba5f-0ba51fb2d108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_black_blob(image, blob_size=2):\n",
    "    image = image.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    center = (width // 2, height // 2)\n",
    "    top_left = (center[0] - blob_size, center[1] - blob_size)\n",
    "    bottom_right = (center[0] + blob_size, center[1] + blob_size)\n",
    "    draw.rectangle([top_left, bottom_right], fill='black')\n",
    "    return image\n",
    "\n",
    "def add_edge_fade(image, fade_color=(0, 0, 0), fade_size=30):\n",
    "    # Convert the image to grayscale to create a radial gradient mask\n",
    "    width, height = image.size\n",
    "    x = np.linspace(-1, 1, width)\n",
    "    y = np.linspace(-1, 1, height)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    distance = np.sqrt(xx**2 + yy**2)\n",
    "\n",
    "    # Create a radial fade mask (distance scaled to fade_size)\n",
    "    mask = np.clip(1 - distance * fade_size, 0, 1)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the grayscale mask to an image\n",
    "    mask_image = Image.fromarray(mask)\n",
    "\n",
    "    # Create a solid image with the fade color\n",
    "    fade_image = Image.new(\"RGB\", (width, height), fade_color)\n",
    "\n",
    "    # Composite the original image and the fade image using the mask\n",
    "    faded_image = Image.composite(image, fade_image, mask_image)\n",
    "\n",
    "    return faded_image\n",
    "\n",
    "# Function to adjust the contrast of an image\n",
    "def adjust_contrast(image, contrast_factor=1.2):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    return enhancer.enhance(contrast_factor)\n",
    "\n",
    "# Define a simple MLP model (as in your original code)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Helper function to calculate confidence intervals\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)  # Standard Error of the Mean\n",
    "    margin = sem * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93617283-a201-4075-8788-8995d91af3ec",
   "metadata": {},
   "source": [
    "# Implement Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a446bfce-5c0a-4a84-850d-9415dd29f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2493007/3047755168.py:205: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  r2 = 1 - (sse / tss)\n",
      "/tmp/ipykernel_2493007/3047755168.py:281: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  r2 = 1 - (sse / tss)\n",
      "/tmp/ipykernel_2493007/3047755168.py:335: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  r2 = 1 - (sse / tss)\n",
      "/n/home07/fucheng/.conda/envs/hface/lib/python3.11/site-packages/numpy/core/_methods.py:173: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cr_mean': -inf, 'normal_32_mean': -inf, 'normal_256_mean': -inf, 'sample_mean': -inf, 'cr_margin': nan, 'normal_32_margin': nan, 'normal_256_margin': nan, 'sample_margin': nan, 'max_diff_ratio': -inf, 'max_diff_sample_ratio': -inf, 'sample_cr_ratio': -inf}\n",
      "{'cr_mean': 0.14898893277553354, 'normal_32_mean': -0.006074598258624719, 'normal_256_mean': 0.19478255659699928, 'sample_mean': 0.24337391961397348, 'cr_margin': 0.006600113316649881, 'normal_32_margin': 0.0017347217561652069, 'normal_256_margin': 0.0018802261224330945, 'sample_margin': 0.04598979828085454, 'max_diff_ratio': -0.23510125660898737, 'max_diff_sample_ratio': 0.24946465364200263, 'sample_cr_ratio': 0.633503342027694}\n",
      "{'cr_mean': 0.3380458924873936, 'normal_32_mean': 0.19024790233107736, 'normal_256_mean': 0.19343875608540195, 'sample_mean': 0.21825046883599103, 'cr_margin': 0.031045338209211952, 'normal_32_margin': 0.012395187183906657, 'normal_256_margin': 0.03442351979322348, 'sample_margin': 0.03035581740052056, 'max_diff_ratio': 0.747560309673148, 'max_diff_sample_ratio': 0.12826650280792162, 'sample_cr_ratio': -0.35437621433566147}\n",
      "{'cr_mean': 0.14391342856544437, 'normal_32_mean': 0.06542266721520487, 'normal_256_mean': 0.10231996661774176, 'sample_mean': 0.1185676742175475, 'cr_margin': 0.01840055490610074, 'normal_32_margin': 0.0010672701733438301, 'normal_256_margin': 0.008544996983294995, 'sample_margin': 0.0177696175879473, 'max_diff_ratio': 0.4065038654976507, 'max_diff_sample_ratio': 0.15879312842727686, 'sample_cr_ratio': -0.17611806348127498}\n",
      "{'cr_mean': 0.15497748334182807, 'normal_32_mean': 0.22177495545310918, 'normal_256_mean': 0.0008029232050103085, 'sample_mean': 0.0016028905832225313, 'cr_margin': 0.006593871656218806, 'normal_32_margin': 0.04652487255005112, 'normal_256_margin': 9.825596719196346e-05, 'sample_margin': 0.0007869447079372897, 'max_diff_ratio': -0.3011948394930662, 'max_diff_sample_ratio': -0.9927724454731706, 'sample_cr_ratio': -0.9896572679549385}\n",
      "{'cr_mean': 0.11953751508119219, 'normal_32_mean': 0.08555567614498491, 'normal_256_mean': 0.07256302243055009, 'sample_mean': 0.0840676677479429, 'cr_margin': 0.011761582746307913, 'normal_32_margin': 0.014066379693744144, 'normal_256_margin': 0.0015910760795092907, 'sample_margin': 0.011667446940205845, 'max_diff_ratio': 0.3971897653946508, 'max_diff_sample_ratio': -0.01739228142526033, 'sample_cr_ratio': -0.2967256539435129}\n",
      "{'cr_mean': 0.1120842924507597, 'normal_32_mean': 0.09339947682751468, 'normal_256_mean': 0.05776627968652785, 'sample_mean': 0.06589037325147416, 'cr_margin': 0.0010660755288544803, 'normal_32_margin': 0.014928454223731936, 'normal_256_margin': 0.00594725373031366, 'sample_margin': 0.010147928329884674, 'max_diff_ratio': 0.20005267971416124, 'max_diff_sample_ratio': -0.2945316666692138, 'sample_cr_ratio': -0.41213552933457837}\n",
      "{'cr_mean': 0.09023191422757659, 'normal_32_mean': 0.023948040850491258, 'normal_256_mean': 0.04149312726520045, 'sample_mean': 0.05346323959744822, 'cr_margin': 0.0003945714560352159, 'normal_32_margin': 0.03008745954921956, 'normal_256_margin': 0.014856263339059326, 'sample_margin': 0.011209141083062982, 'max_diff_ratio': 1.1746231285693547, 'max_diff_sample_ratio': 0.28848421705458893, 'sample_cr_ratio': -0.4074907968525749}\n",
      "{'cr_mean': 0.01342593615743679, 'normal_32_mean': 4.3495027489992544e-05, 'normal_256_mean': 0.008146009245974063, 'sample_mean': 0.012590395156993076, 'cr_margin': 0.0012377851459520306, 'normal_32_margin': 0.0014590921372611544, 'normal_256_margin': 0.003032740998639471, 'sample_margin': 0.0029999333184325197, 'max_diff_ratio': 0.6481611734079706, 'max_diff_sample_ratio': 0.5455905802237488, 'sample_cr_ratio': -0.06223335122749698}\n",
      "{'cr_mean': 0.6219876698096097, 'normal_32_mean': 0.10023024924512504, 'normal_256_mean': 0.5498509529156483, 'sample_mean': 0.5775958417080276, 'cr_margin': 0.017389831132976794, 'normal_32_margin': 0.00316071841561992, 'normal_256_margin': 0.024994083628862667, 'sample_margin': 0.018626410468022436, 'max_diff_ratio': 0.13119321974700254, 'max_diff_sample_ratio': 0.05045892645135697, 'sample_cr_ratio': -0.07137091337384627}\n",
      "{'cr_mean': 0.5643681085254554, 'normal_32_mean': 0.4279535127604718, 'normal_256_mean': 0.5221069750542184, 'sample_mean': 0.5435721943470837, 'cr_margin': 0.016068827026327424, 'normal_32_margin': 0.013222042203955332, 'normal_256_margin': 0.020905073350190526, 'sample_margin': 0.02196123328412472, 'max_diff_ratio': 0.08094343782104868, 'max_diff_sample_ratio': 0.04111268440847051, 'sample_cr_ratio': -0.0368481384121897}\n",
      "{'cr_mean': 0.14827668253571302, 'normal_32_mean': 0.07330038195242063, 'normal_256_mean': 0.12174532811281225, 'sample_mean': 0.12641226031278613, 'cr_margin': 0.03067890041521443, 'normal_32_margin': 0.013983665406338852, 'normal_256_margin': 0.015878319465244577, 'sample_margin': 0.027351353352662394, 'max_diff_ratio': 0.2179250311627249, 'max_diff_sample_ratio': 0.03833356295733491, 'sample_cr_ratio': -0.1474569153356986}\n",
      "{'cr_mean': 0.5864103539255484, 'normal_32_mean': 0.42450436904218225, 'normal_256_mean': 0.2755927173185487, 'sample_mean': 0.2489365270766705, 'cr_margin': 0.008965799732992753, 'normal_32_margin': 0.00733192578959304, 'normal_256_margin': 0.00717802105968059, 'sample_margin': 0.023965769816707817, 'max_diff_ratio': 0.3814000436525021, 'max_diff_sample_ratio': -0.41358312132722924, 'sample_cr_ratio': -0.575490907672009}\n",
      "{'cr_mean': 0.1247561957786345, 'normal_32_mean': 0.07124381391375761, 'normal_256_mean': 0.07007956337646548, 'sample_mean': 0.07886924017751043, 'cr_margin': 0.0008610107002874103, 'normal_32_margin': 0.006871221193410153, 'normal_256_margin': 0.0060402834537286905, 'sample_margin': 0.015403239783822336, 'max_diff_ratio': 0.7511161871493144, 'max_diff_sample_ratio': 0.10703281934040731, 'sample_cr_ratio': -0.36781303978317187}\n",
      "{'cr_mean': 0.12025550014259377, 'normal_32_mean': 0.10745084508090358, 'normal_256_mean': 0.06216079561450322, 'sample_mean': 0.06475275821145435, 'cr_margin': 0.022020936574797763, 'normal_32_margin': 0.033427304798905516, 'normal_256_margin': 0.033715051717706335, 'sample_margin': 0.011126957947809987, 'max_diff_ratio': 0.11916756031140668, 'max_diff_sample_ratio': -0.397373206672319, 'sample_cr_ratio': -0.4615401529686931}\n",
      "{'cr_mean': 0.06656523586843377, 'normal_32_mean': 0.04550401942329391, 'normal_256_mean': 0.050634846311180336, 'sample_mean': 0.05350128907797116, 'cr_margin': 0.006212178104900009, 'normal_32_margin': 0.009391650393662623, 'normal_256_margin': 0.005019097779544971, 'sample_margin': 0.00780223230181493, 'max_diff_ratio': 0.31461317092486074, 'max_diff_sample_ratio': 0.05661008130991211, 'sample_cr_ratio': -0.19625780063761075}\n",
      "\n",
      "DataFrame Preview:\n",
      "                     ROTATION  BLOB  CONTRAST  EDGE   cr_mean  normal_32_mean  \\\n",
      "ROT0_BLO0_CON0_EDG0         0     0         0     0      -inf            -inf   \n",
      "ROT0_BLO0_CON0_EDG1         0     0         0     1  0.148989       -0.006075   \n",
      "ROT0_BLO0_CON1_EDG0         0     0         1     0  0.338046        0.190248   \n",
      "ROT0_BLO0_CON1_EDG1         0     0         1     1  0.143913        0.065423   \n",
      "ROT0_BLO1_CON0_EDG0         0     1         0     0  0.154977        0.221775   \n",
      "\n",
      "                     normal_256_mean  sample_mean  cr_margin  \\\n",
      "ROT0_BLO0_CON0_EDG0             -inf         -inf        NaN   \n",
      "ROT0_BLO0_CON0_EDG1         0.194783     0.243374   0.006600   \n",
      "ROT0_BLO0_CON1_EDG0         0.193439     0.218250   0.031045   \n",
      "ROT0_BLO0_CON1_EDG1         0.102320     0.118568   0.018401   \n",
      "ROT0_BLO1_CON0_EDG0         0.000803     0.001603   0.006594   \n",
      "\n",
      "                     normal_32_margin  normal_256_margin  sample_margin  \\\n",
      "ROT0_BLO0_CON0_EDG0               NaN                NaN            NaN   \n",
      "ROT0_BLO0_CON0_EDG1          0.001735           0.001880       0.045990   \n",
      "ROT0_BLO0_CON1_EDG0          0.012395           0.034424       0.030356   \n",
      "ROT0_BLO0_CON1_EDG1          0.001067           0.008545       0.017770   \n",
      "ROT0_BLO1_CON0_EDG0          0.046525           0.000098       0.000787   \n",
      "\n",
      "                     max_diff_ratio  max_diff_sample_ratio  sample_cr_ratio  \n",
      "ROT0_BLO0_CON0_EDG0            -inf                   -inf             -inf  \n",
      "ROT0_BLO0_CON0_EDG1       -0.235101               0.249465         0.633503  \n",
      "ROT0_BLO0_CON1_EDG0        0.747560               0.128267        -0.354376  \n",
      "ROT0_BLO0_CON1_EDG1        0.406504               0.158793        -0.176118  \n",
      "ROT0_BLO1_CON0_EDG0       -0.301195              -0.992772        -0.989657  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate mean and confidence interval\"\"\"\n",
    "    if not data:  # Check if data list is empty\n",
    "        return float('-inf'), 0\n",
    "    \n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    if len(data) < 2:  # Need at least 2 samples for std calculation\n",
    "        return mean, 0\n",
    "        \n",
    "    stderr = np.std(data, ddof=1) / np.sqrt(len(data))\n",
    "    interval = stderr * 1.96  # For 95% confidence interval\n",
    "    return mean, interval\n",
    "\n",
    "# Define variables and their possible values\n",
    "variables = ['ROTATION', 'BLOB', 'CONTRAST', 'EDGE']\n",
    "values = [0, 1]\n",
    "\n",
    "# Generate all possible combinations\n",
    "combinations = list(product(values, repeat=len(variables)))\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Create readable index names\n",
    "index_names = []\n",
    "\n",
    "for combo in combinations:\n",
    "    # Reset data collection for each combination\n",
    "    normal_r2s = [[], []]  # One list for each image size\n",
    "    sampled_r2s = []\n",
    "    cr_r2s = []\n",
    "    \n",
    "    # Create a dictionary mapping variables to their values\n",
    "    params = dict(zip(variables, combo))\n",
    "    ROTATION = params['ROTATION']\n",
    "    BLOB = params['BLOB']\n",
    "    CONTRAST = params['CONTRAST']\n",
    "    EDGE = params['EDGE']\n",
    "    \n",
    "    # Create readable index name\n",
    "    index_name = '_'.join([f\"{var[:3]}{val}\" for var, val in params.items()])\n",
    "    \n",
    "    for seed in [10, 20]:\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Total number of images\n",
    "        n_images = len(pil_images_list[0])\n",
    "\n",
    "        blob_percentage = 0.5\n",
    "        rotate_percentage = 0.5\n",
    "\n",
    "        num_blob = int(n_images * blob_percentage)\n",
    "        num_rotate = int(n_images * rotate_percentage)\n",
    "\n",
    "        # Generate unique random indices\n",
    "        rotation_indices = random.sample(range(n_images), num_rotate)\n",
    "        blob_indices = random.sample(range(n_images), num_blob)\n",
    "        edge_indices = random.sample(range(n_images), num_blob)\n",
    "        contrast_indices = random.sample(range(n_images), num_blob)\n",
    "\n",
    "        # Generate synthetic outcomes\n",
    "        synthetic_outcomes = np.zeros(len(pil_images_list[0]))\n",
    "\n",
    "        if ROTATION:\n",
    "            synthetic_outcomes[rotation_indices] += np.random.normal(loc=-100.0, scale=10.0, size=len(rotation_indices))\n",
    "        if BLOB:\n",
    "            synthetic_outcomes[blob_indices] += np.random.normal(loc=100.0, scale=10.0, size=len(blob_indices))\n",
    "        if EDGE:\n",
    "            synthetic_outcomes[edge_indices] += np.random.normal(loc=100.0, scale=0.0, size=len(edge_indices))\n",
    "        if CONTRAST:\n",
    "            synthetic_outcomes[contrast_indices] += np.random.normal(loc=100.0, scale=10.0, size=len(contrast_indices))\n",
    "\n",
    "        augmented_images_list = copy.deepcopy(pil_images_list)\n",
    "\n",
    "        # Apply image augmentations\n",
    "        for i in range(NUM_IMAGE_SIZES):\n",
    "            if BLOB:\n",
    "                for idx in blob_indices:\n",
    "                    augmented_images_list[i][idx] = add_black_blob(augmented_images_list[i][idx], blob_size=1)\n",
    "\n",
    "            if EDGE and i == 1:\n",
    "                for idx in edge_indices:\n",
    "                    augmented_images_list[i][idx] = add_edge_fade(augmented_images_list[i][idx].copy(), \n",
    "                                                                fade_color=(0, 0, 0), fade_size=0.5)\n",
    "\n",
    "            if ROTATION:\n",
    "                for idx in rotation_indices:\n",
    "                    augmented_images_list[i][idx] = augmented_images_list[i][idx].rotate(90, expand=True)\n",
    "\n",
    "            if CONTRAST:\n",
    "                for idx in contrast_indices:\n",
    "                    augmented_images_list[i][idx] = adjust_contrast(augmented_images_list[i][idx], contrast_factor=1.8)\n",
    "\n",
    "        # Get embeddings for all images\n",
    "        all_image_embeddings = defaultdict(list)\n",
    "        for i, image_size in enumerate(image_sizes):\n",
    "            pil_images_app = augmented_images_list[i]\n",
    "            batch_size = 64\n",
    "\n",
    "            for j in range(0, len(pil_images_app), batch_size):\n",
    "                batch_images = pil_images_app[j:j+batch_size]\n",
    "                image_preprocessed = backbone_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if CLIP:\n",
    "                        image_embeddings = backbone_model.get_image_features(**image_preprocessed)\n",
    "                    elif SWIN:\n",
    "                        outputs = model(**inputs)\n",
    "                        last_hidden_states = outputs.last_hidden_state.detach()\n",
    "                        image_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "                all_image_embeddings[image_size].append(image_embeddings)\n",
    "\n",
    "            all_image_embeddings[image_size] = torch.cat(all_image_embeddings[image_size], dim=0)\n",
    "            \n",
    "        # Get embeddings for all images\n",
    "        sampled_images_embeddings = defaultdict(list)\n",
    "        for monte_i in monte_is:\n",
    "            if monte_i == 1:\n",
    "                continue\n",
    "                \n",
    "            pil_images_app = sampled_images_list[i]\n",
    "            batch_size = 64\n",
    "\n",
    "            for j in range(0, len(pil_images_app), batch_size):\n",
    "                batch_images = pil_images_app[j:j+batch_size]\n",
    "                image_preprocessed = backbone_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if CLIP:\n",
    "                        image_embeddings = backbone_model.get_image_features(**image_preprocessed)\n",
    "                    elif SWIN:\n",
    "                        outputs = model(**inputs)\n",
    "                        last_hidden_states = outputs.last_hidden_state.detach()\n",
    "                        image_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "                sampled_images_embeddings[monte_i].append(image_embeddings)\n",
    "\n",
    "            sampled_images_embeddings[monte_i] = torch.cat(sampled_images_embeddings[monte_i], dim=0)\n",
    "\n",
    "\n",
    "        # Initialize KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Single resolution training\n",
    "        mse_list = defaultdict(list)\n",
    "        r2_list = defaultdict(list)\n",
    "\n",
    "        for i, image_size in enumerate(image_sizes):\n",
    "            X = all_image_embeddings[image_size]\n",
    "            Y = synthetic_outcomes\n",
    "\n",
    "            if torch.is_tensor(X):\n",
    "                X = X.cpu().numpy()\n",
    "            if torch.is_tensor(Y):\n",
    "                Y = Y.cpu().numpy()\n",
    "\n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "                # Convert to tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
    "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "                Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Train model\n",
    "                input_size = X_train.shape[1]\n",
    "                model = MLP(input_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(100):\n",
    "                    model.train()\n",
    "                    Y_pred_train = model(X_train_tensor)\n",
    "                    loss = loss_fn(Y_pred_train, Y_train_tensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    Y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "                # Calculate metrics\n",
    "                sse = np.sum((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "                mse = np.mean((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "                tss = np.sum((Y_test - Y_test.mean())**2)\n",
    "                r2 = 1 - (sse / tss)\n",
    "\n",
    "                mse_list[image_size].append(mse)\n",
    "                r2_list[image_size].append(r2)\n",
    "\n",
    "            # Store average R² for this image size\n",
    "            if r2_list[image_size]:\n",
    "                normal_r2s[i].append(np.mean(r2_list[image_size]))\n",
    "\n",
    "                \n",
    "                \n",
    "        # Initialize KFold for Sampling Training\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        mse_sample = []\n",
    "        r2_sample = []\n",
    "\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Extract and collect embeddings for each `monte_i`\n",
    "        \n",
    "        for j in range(len(monte_is)):\n",
    "            monte_i = j\n",
    "            if monte_i == 1: \n",
    "                continue\n",
    "            X = sampled_images_embeddings[monte_i]\n",
    "            if torch.is_tensor(X):\n",
    "                X = X.cpu().numpy()\n",
    "            embeddings_list.append(X)\n",
    "            embeddings_matrix = np.column_stack(embeddings_list)\n",
    "            X = all_image_embeddings[256].cpu().numpy() if torch.is_tensor(all_image_embeddings[256]) else all_image_embeddings[256]\n",
    "            X2 = all_image_embeddings[32].cpu().numpy() if torch.is_tensor(all_image_embeddings[32]) else all_image_embeddings[32]\n",
    "            embeddings_list = [embeddings_matrix, X]\n",
    "            X = np.column_stack(embeddings_list)\n",
    "\n",
    "            Y = synthetic_outcomes\n",
    "\n",
    "            if torch.is_tensor(X):\n",
    "                X = X.cpu().numpy()\n",
    "            if torch.is_tensor(Y):\n",
    "                Y = Y.cpu().numpy()\n",
    "\n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "                # Convert to tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
    "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "                Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                # Train model\n",
    "                input_size = X_train.shape[1]\n",
    "                model = MLP(input_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(100):\n",
    "                    model.train()\n",
    "                    Y_pred_train = model(X_train_tensor)\n",
    "                    loss = loss_fn(Y_pred_train, Y_train_tensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    Y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "                # Calculate metrics\n",
    "                sse = np.sum((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "                mse = np.mean((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "                tss = np.sum((Y_test - Y_test.mean())**2)\n",
    "                r2 = 1 - (sse / tss)\n",
    "\n",
    "                mse_sample.append(mse)\n",
    "                r2_sample.append(r2)\n",
    "\n",
    "            # Store average R² for this image size\n",
    "            if r2_sample:\n",
    "                sampled_r2s.append(np.mean(r2_sample))\n",
    "\n",
    "\n",
    "        # Cross-resolution training\n",
    "        X_list = [all_image_embeddings[image_size] for image_size in image_sizes]\n",
    "        X = torch.cat(X_list, dim=1)\n",
    "        \n",
    "        if torch.is_tensor(X):\n",
    "            X = X.cpu().numpy()\n",
    "\n",
    "        mse_cross = []\n",
    "        r2_cross = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            # Convert to tensors\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            # Train model\n",
    "            input_size = X_train.shape[1]\n",
    "            model = MLP(input_size)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(100):\n",
    "                model.train()\n",
    "                Y_pred_train = model(X_train_tensor)\n",
    "                loss = loss_fn(Y_pred_train, Y_train_tensor)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "            # Calculate metrics\n",
    "            sse = np.sum((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "            mse = np.mean((Y_test - Y_pred_test.squeeze(1))**2)\n",
    "            tss = np.sum((Y_test - Y_test.mean())**2)\n",
    "            r2 = 1 - (sse / tss)\n",
    "\n",
    "            mse_cross.append(mse)\n",
    "            r2_cross.append(r2)\n",
    "\n",
    "        if r2_cross:\n",
    "            cr_r2s.append(np.mean(r2_cross))\n",
    "\n",
    "    # Calculate final statistics\n",
    "    sample_mean, sample_margin = mean_confidence_interval(sampled_r2s)\n",
    "    cr_mean, cr_margin = mean_confidence_interval(cr_r2s)\n",
    "    normal_32_mean, normal_32_margin = mean_confidence_interval(normal_r2s[0])\n",
    "    normal_256_mean, normal_256_margin = mean_confidence_interval(normal_r2s[1])\n",
    "\n",
    "    # Calculate max mean and difference ratio\n",
    "    valid_means = [m for m in [normal_32_mean, normal_256_mean] if m != float('-inf')]\n",
    "    max_mean = max(valid_means) if valid_means else float('-inf')\n",
    "    \n",
    "    if max_mean != float('-inf') and max_mean != 0:\n",
    "        max_diff_ratio = (cr_mean - max_mean) / max_mean\n",
    "        max_diff_sample_ratio = (sample_mean - max_mean) / max_mean\n",
    "        sample_cr_ratio = (sample_mean - cr_mean)/cr_mean\n",
    "    else:\n",
    "        max_diff_ratio = float('-inf')\n",
    "        max_diff_sample_ratio = float('-inf')\n",
    "        sample_cr_ratio = float('-inf')\n",
    "\n",
    "    # Store results\n",
    "    experiment_results = {\n",
    "        'cr_mean': cr_mean,\n",
    "        'normal_32_mean': normal_32_mean,\n",
    "        'normal_256_mean': normal_256_mean,\n",
    "        'sample_mean': sample_mean,\n",
    "        'cr_margin': cr_margin,\n",
    "        'normal_32_margin': normal_32_margin,\n",
    "        'normal_256_margin': normal_256_margin,\n",
    "        'sample_margin': sample_margin,\n",
    "        'max_diff_ratio': max_diff_ratio, \n",
    "        'max_diff_sample_ratio': max_diff_sample_ratio,\n",
    "        'sample_cr_ratio': sample_cr_ratio\n",
    "    }\n",
    "    \n",
    "    print(experiment_results)\n",
    "    \n",
    "    result_row = {**params, **experiment_results}\n",
    "    results.append(result_row)\n",
    "    index_names.append(index_name)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results, index=index_names)\n",
    "\n",
    "# Reorder columns\n",
    "param_cols = variables\n",
    "result_cols = ['cr_mean', 'normal_32_mean', 'normal_256_mean', 'sample_mean', \n",
    "               'cr_margin', 'normal_32_margin', 'normal_256_margin', 'sample_margin',\n",
    "               'max_diff_ratio', 'max_diff_sample_ratio', 'sample_cr_ratio']\n",
    "df = df[param_cols + result_cols]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDataFrame Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save results\n",
    "df.to_csv('experiment_results_sample_32_and_256_3to7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c65c322-437e-4a1e-b1e5-19288b6f7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('experiment_results_sample_32_and_256_3to7.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalImagesEnv_arm64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
